{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, balanced_accuracy_score\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn.model_selection import KFold, RepeatedStratifiedKFold, StratifiedKFold\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#APPAC, PAAC, AAC, & DPC\n",
    "X1=pd.read_csv('Features/Sodium_APAAC.csv', header=None).iloc[:,1:].values\n",
    "X2=pd.read_csv('Features/Sodium_PAAC.csv', header=None).iloc[:,1:].values\n",
    "X3=pd.read_csv('Features/Sodium_AAC.csv', header=None).iloc[:,1:].values\n",
    "X4=pd.read_csv('Features/Sodium_DPC.csv', header=None).iloc[:,1:].values\n",
    "\n",
    "#Labels\n",
    "pos_labels = np.ones(492)\n",
    "neg_labels = np.zeros(492)\n",
    "y = np.concatenate((pos_labels,neg_labels),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=np.concatenate((X1,X2, X3, X4), axis=1, out=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test= train_test_split(X, y, test_size=0.2, random_state=67, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #NXTPred data\n",
    "# N1=pd.read_csv('Data/Sodium/NTXPred/PREDNa/Features/NTXPred_CKSAAGP.csv', header=None).iloc[:,1:].values\n",
    "# N2=pd.read_csv('Data/Sodium/NTXPred/PREDNa/Features/NTXPred_PAAC.csv', header=None).iloc[:,1:].values\n",
    "# N3=pd.read_csv('Data/Sodium/NTXPred/PREDNa/Features/NTXPred_APAAC.csv', header=None).iloc[:,1:].values\n",
    "# N4=pd.read_csv('Data/Sodium/NTXPred/PREDNa/Features/NTXPred_GTPC.csv', header=None).iloc[:,1:].values\n",
    "\n",
    "# #Labels\n",
    "# y1= np.ones(244)\n",
    "# y2=np.zeros(244)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train=np.concatenate((N1, N2, N3, N4), axis=1, out=None)\n",
    "# y=np.concatenate((y1,y2),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Keep almost all the samples in the training set to check CV results \n",
    "# X_train2, X_test2, y_train2, y_test2= train_test_split(train, y, test_size=0.005, random_state=67, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_test(model, X_test, y_test):\n",
    "    from sklearn import metrics\n",
    "\n",
    "    # Predict Test Data \n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Calculate accuracy, precision, recall, f1-score, and kappa score\n",
    "    acc = metrics.accuracy_score(y_test, y_pred)\n",
    "    prec = metrics.precision_score(y_test, y_pred)\n",
    "    rec = metrics.recall_score(y_test, y_pred)\n",
    "    f1 = metrics.f1_score(y_test, y_pred)\n",
    "\n",
    "    # Calculate area under curve (AUC)\n",
    "    y_pred_proba = model.predict_proba(X_test)[::,1]\n",
    "    fpr, tpr, _ = metrics.roc_curve(y_test, y_pred_proba)\n",
    "    auc = metrics.roc_auc_score(y_test, y_pred_proba)\n",
    "    \n",
    "    #MCC\n",
    "    mcc=matthews_corrcoef(y_test, model.predict(X_test))\n",
    "    \n",
    "    # Display confussion matrix\n",
    "    cm = metrics.confusion_matrix(y_test, y_pred)\n",
    "    total=sum(sum(cm))\n",
    "    \n",
    "    #accuracy=(cm[0,0]+cm[1,1])/total\n",
    "    spec= cm[0,0]/(cm[0,0]+cm[0,1])\n",
    "    sen= cm[1,1]/(cm[1,0]+cm[1,1])\n",
    "\n",
    "    return {'acc': acc, 'prec': prec, 'rec': rec, 'f1': f1, 'mcc':mcc,\n",
    "            'fpr': fpr, 'tpr': tpr, 'auc': auc, 'cm': cm, 'sen': sen, 'spec':spec}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "def evaluate_model_train(model, X_train, y_train):\n",
    "    conf_matrix_list_of_arrays = []\n",
    "    mcc_array=[]\n",
    "    cv = KFold(n_splits=5) \n",
    "    lst_accu = []\n",
    "    AUC_list=[]\n",
    "    \n",
    "    score=cross_val_score(model, X_train, y_train, cv=cv, scoring='accuracy',n_jobs=-1, error_score='raise')\n",
    "    prec_train=np.mean(cross_val_score(model, X_train, y_train, cv=cv, scoring='precision'))\n",
    "    recall_train=np.mean(cross_val_score(model, X_train, y_train, cv=cv, scoring='recall'))\n",
    "    f1_train=np.mean(cross_val_score(model, X_train, y_train, cv=cv, scoring='f1'))\n",
    "    \n",
    "    for train_index, test_index in cv.split(X_train, y_train): \n",
    "        X_train_fold, X_test_fold = X_train[train_index], X_train[test_index] \n",
    "        y_train_fold, y_test_fold = y_train[train_index], y_train[test_index] \n",
    "        model.fit(X_train_fold, y_train_fold) \n",
    "        lst_accu.append(model.score(X_test_fold, y_test_fold))\n",
    "        conf_matrix = confusion_matrix(y_test_fold, model.predict(X_test_fold))\n",
    "        conf_matrix_list_of_arrays.append(conf_matrix)\n",
    "        cm = np.mean(conf_matrix_list_of_arrays, axis=0)\n",
    "        mcc_array.append(matthews_corrcoef(y_test_fold, model.predict(X_test_fold)))\n",
    "        mcc=np.mean(mcc_array, axis=0)\n",
    "        \n",
    "        # Calculate area under curve (AUC)\n",
    "        AUC=metrics.roc_auc_score( y_test_fold, model.predict_proba(X_test_fold)[:,1])\n",
    "        AUC_list.append(AUC)\n",
    "        auc=np.mean(AUC_list)\n",
    "        \n",
    "        \n",
    "    total=sum(sum(cm))\n",
    "    accuracy=(cm[0,0]+cm[1,1])/total\n",
    "    specificity= cm[0,0]/(cm[0,0]+cm[0,1])\n",
    "    sensitivity= cm[1,1]/(cm[1,0]+cm[1,1])\n",
    "    \n",
    "    \n",
    "    return {'prec_train': prec_train, 'recall_train': recall_train,\n",
    "            'f1_train': f1_train, 'cm': cm, 'mcc': mcc,'acc':accuracy,\n",
    "           'sen':sensitivity,'spec':specificity, 'AUC':auc, 'score':score}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "def RF_objective(trial):\n",
    "    n_estimators = trial.suggest_int(\"n_estimators\", 100, 2000)\n",
    "    max_depth = trial.suggest_int('max_depth', 1, 80)\n",
    "    max_leaf_nodes = trial.suggest_int('max_leaf_nodes', 2, 1000)\n",
    "    min_samples_split= trial.suggest_int(\"min_samples_split\", 2, 20)\n",
    "    \n",
    "    ## Create Model\n",
    "    model = RandomForestClassifier(max_depth = max_depth, min_samples_split=min_samples_split,\n",
    "                                   n_estimators = n_estimators,n_jobs=2\n",
    "                                     )\n",
    "\n",
    "\n",
    "    score = cross_val_score(model, X_train, y_train, cv=5, scoring=\"accuracy\")\n",
    "    accuracy_mean = score.mean()\n",
    "    return accuracy_mean\n",
    "\n",
    "#Execute optuna and set hyperparameters\n",
    "RF_study = optuna.create_study(direction='maximize')\n",
    "RF_study.optimize(RF_objective, n_trials=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_RF=RandomForestClassifier(**RF_study.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Model on Training data\n",
    "train_eval = evaluate_model_train(optimized_RF, X_train, y_train)\n",
    "print(\"Confusion Matrix is: \", train_eval['cm'])\n",
    "print ('Accuracy : ', train_eval['acc'])\n",
    "print('Sensitivity : ', train_eval['sen'])\n",
    "print('Specificity : ', train_eval['spec'])\n",
    "print(\"Mean of Matthews Correlation Coefficient is: \", train_eval['mcc'])\n",
    "print(\"The Acc value from CM is: \", train_eval['acc'])\n",
    "print(\"The Recall value is: \", train_eval['recall_train'])\n",
    "print(\"The F1 score is: \", train_eval['f1_train'])\n",
    "print('The area under curve is:', train_eval['AUC'])\n",
    "#print('5 accuracies: ', train_eval['lst_accu'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Model on Testing data\n",
    "dtc_eval = evaluate_model_test(optimized_RF, X_test, y_test)\n",
    "# Print result\n",
    "print('Accuracy:', dtc_eval['acc'])\n",
    "print('Precision:', dtc_eval['prec'])\n",
    "print('Recall:', dtc_eval['rec'])\n",
    "print('F1 Score:', dtc_eval['f1'])\n",
    "print('Area Under Curve:', dtc_eval['auc'])\n",
    "print('Sensitivity : ', dtc_eval['sen'])\n",
    "print('Specificity : ', dtc_eval['spec'])\n",
    "print('MCC Score : ', dtc_eval['mcc'])\n",
    "print('Confusion Matrix:\\n', dtc_eval['cm'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "import optuna\n",
    "def objective(trial):\n",
    "    \"\"\"Define the objective function\"\"\"\n",
    "    params = {\n",
    "            'n_estimators' : trial.suggest_int('n_estimators', 100, 2000),\n",
    "            'max_depth' : trial.suggest_int('max_depth', 10, 90),\n",
    "            'max_leaf_nodes' : trial.suggest_int('max_leaf_nodes', 15, 100),\n",
    "            'criterion' : trial.suggest_categorical('criterion', ['gini', 'entropy'])\n",
    "\n",
    "    }\n",
    "\n",
    "\n",
    "    # Fit the model\n",
    "    etc_model = ExtraTreesClassifier(**params)\n",
    "    \n",
    "    score = cross_val_score(etc_model, X_train, y_train, cv=5, scoring=\"accuracy\")\n",
    "    accuracy_mean = score.mean()\n",
    "\n",
    "    return accuracy_mean\n",
    "\n",
    "\n",
    "#Execute optuna and set hyperparameters\n",
    "etc_study = optuna.create_study(direction='maximize')\n",
    "etc_study.optimize(objective, n_trials=20)\n",
    "\n",
    "optimized_etc =ExtraTreesClassifier(**etc_study.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Model on Training data\n",
    "train_eval = evaluate_model_train(optimized_etc, X_train, y_train)\n",
    "print(\"Confusion Matrix is:\\n\", train_eval['cm'])\n",
    "print ('Accuracy : ', train_eval['acc'])\n",
    "print('Sensitivity : ', train_eval['sen'])\n",
    "print('Specificity : ', train_eval['spec'])\n",
    "print(\"Matthews Correlation Coefficient is: \", train_eval['mcc'])\n",
    "print(\"Precision value is: \", train_eval['prec_train'])\n",
    "print(\"Recall value is: \", train_eval['recall_train'])\n",
    "print('The area under curve is:', train_eval['AUC'])\n",
    "print(\"F1 score is: \", train_eval['f1_train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Model on Testing data\n",
    "dtc_eval = evaluate_model_test(optimized_etc, X_test, y_test)\n",
    "# Print result\n",
    "print('Accuracy:', dtc_eval['acc'])\n",
    "print('Precision:', dtc_eval['prec'])\n",
    "print('Recall:', dtc_eval['rec'])\n",
    "print('F1 Score:', dtc_eval['f1'])\n",
    "print('Area Under Curve:', dtc_eval['auc'])\n",
    "print('Sensitivity : ', dtc_eval['sen'])\n",
    "print('Specificity : ', dtc_eval['spec'])\n",
    "print('MCC Score : ', dtc_eval['mcc'])\n",
    "print('Confusion Matrix:\\n', dtc_eval['cm'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "#cv = RepeatedStratifiedKFold(n_splits=5)\n",
    "import optuna\n",
    "def objective(trial):\n",
    "    \"\"\"Define the objective function\"\"\"\n",
    "\n",
    "    params = {\n",
    "        'max_depth': trial.suggest_int('max_depth', 1, 370),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.001, 10.0),\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 50, 1000),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 50),\n",
    "        'gamma': trial.suggest_float('gamma', 1e-8, 10.0),\n",
    "        'subsample': trial.suggest_float('subsample', 0.01, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.01, 1.0),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 1.0),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 1.0),\n",
    "        #'eval_metric': 'mlogloss',\n",
    "        #'use_label_encoder': False\n",
    "    }\n",
    "\n",
    "    # Fit the model\n",
    "    xgb_model = XGBClassifier(**params,  eval_metric='mlogloss')\n",
    "    score = cross_val_score(xgb_model, X_train, y_train, cv=5, scoring=\"accuracy\")\n",
    "    accuracy_mean = score.mean()\n",
    "    return accuracy_mean\n",
    "#Execute optuna and set hyperparameters\n",
    "XGB_study = optuna.create_study(direction='maximize')\n",
    "XGB_study.optimize(objective, n_trials=20)\n",
    "optimized_XGB =XGBClassifier(**XGB_study.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Model on Training data\n",
    "train_eval = evaluate_model_train(optimized_XGB, X_train, y_train)\n",
    "\n",
    "print(\"Confusion Matrix is:\\n\", train_eval['cm'])\n",
    "print ('Accuracy : ', train_eval['acc'])\n",
    "print('Sensitivity : ', train_eval['sen'])\n",
    "print('Specificity : ', train_eval['spec'])\n",
    "print(\"Matthews Correlation Coefficient is: \", train_eval['mcc'])\n",
    "print(\"Precision value is: \", train_eval['prec_train'])\n",
    "print(\"Recall value is: \", train_eval['recall_train'])\n",
    "print(\"F1 score is: \", train_eval['f1_train'])\n",
    "print('The area under curve is:', train_eval['AUC'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Model on Testing data\n",
    "dtc_eval = evaluate_model_test(optimized_XGB, X_test, y_test)\n",
    "\n",
    "# Print result\n",
    "print('Accuracy:', dtc_eval['acc'])\n",
    "print('Precision:', dtc_eval['prec'])\n",
    "print('Recall:', dtc_eval['rec'])\n",
    "print('F1 Score:', dtc_eval['f1'])\n",
    "print('Area Under Curve:', dtc_eval['auc'])\n",
    "print('Sensitivity : ', dtc_eval['sen'])\n",
    "print('Specificity : ', dtc_eval['spec'])\n",
    "print('MCC Score : ', dtc_eval['mcc'])\n",
    "print('Confusion Matrix:\\n', dtc_eval['cm'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgbm\n",
    "import optuna\n",
    "def objective(trial):\n",
    "    \"\"\"Define the objective function\"\"\"\n",
    "    params = {\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 2, 100), \n",
    "        'max_depth': trial.suggest_int('max_depth', 1, 100), \n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.0001, 10), \n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 1000), \n",
    "        #'objective': 'multiclass', \n",
    "       # 'class_weight': trial.suggest_categorical('class_weight', ['balanced', None]),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 1, 100), \n",
    "        'subsample': trial.suggest_uniform('subsample', 0.7, 1.0), \n",
    "        'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.7, 1.0),\n",
    "        'reg_alpha': trial.suggest_uniform('reg_alpha', 0.0, 1.0),\n",
    "        'reg_lambda': trial.suggest_uniform('reg_lambda', 0.0, 10.0),\n",
    "        'random_state': 0\n",
    "    }\n",
    "\n",
    "\n",
    "    # Fit the model\n",
    "    lgbm_model = lgbm.LGBMClassifier(**params)\n",
    "    score = cross_val_score(lgbm_model, X_train, y_train, cv=5, scoring=\"accuracy\")\n",
    "    accuracy_mean = score.mean()\n",
    "\n",
    "    return accuracy_mean\n",
    "\n",
    "\n",
    "#Execute optuna and set hyperparameters\n",
    "lgbm_study = optuna.create_study(direction='maximize')\n",
    "lgbm_study.optimize(objective, n_trials=30)\n",
    "\n",
    "optimized_lgbm =lgbm.LGBMClassifier(**lgbm_study.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Model on Training data\n",
    "train_eval = evaluate_model_train(optimized_lgbm, X_train, y_train)\n",
    "print(\"Confusion Matrix is: \", train_eval['cm'])\n",
    "print ('Accuracy : ', train_eval['acc'])\n",
    "print('Sensitivity : ', train_eval['sen'])\n",
    "print('Specificity : ', train_eval['spec'])\n",
    "print(\"Mean of Matthews Correlation Coefficient is: \", train_eval['mcc'])\n",
    "print(\"The Precision value is: \", train_eval['prec_train'])\n",
    "print(\"The Recall value is: \", train_eval['recall_train'])\n",
    "print(\"The F1 score is: \", train_eval['f1_train'])\n",
    "print('The area under curve is:', train_eval['AUC'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Model on Testing data\n",
    "dtc_eval = evaluate_model_test(optimized_lgbm, X_test, y_test)\n",
    "# Print result\n",
    "print('Accuracy:', dtc_eval['acc'])\n",
    "print('Precision:', dtc_eval['prec'])\n",
    "print('Recall:', dtc_eval['rec'])\n",
    "print('F1 Score:', dtc_eval['f1'])\n",
    "print('Area Under Curve:', dtc_eval['auc'])\n",
    "print('Sensitivity : ', dtc_eval['sen'])\n",
    "print('Specificity : ', dtc_eval['spec'])\n",
    "print('MCC Score : ', dtc_eval['mcc'])\n",
    "print('Confusion Matrix:\\n', dtc_eval['cm'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for Optuna\n",
    "from sklearn.svm import SVC\n",
    "def objective(trial):\n",
    "    # C\n",
    "    svc_c = trial.suggest_float('C', 1e0, 1e2)\n",
    "    # kernel\n",
    "    kernel = trial.suggest_categorical('kernel', ['linear', 'poly', 'rbf'])\n",
    "    # SVC\n",
    "    clf = SVC(C=svc_c, kernel=kernel)\n",
    "    score = cross_val_score(clf, X_train, y_train, cv=5, scoring=\"accuracy\")\n",
    "    accuracy_mean = score.mean()\n",
    "\n",
    "    return accuracy_mean\n",
    "\n",
    "\n",
    "#Execute optuna and set hyperparameters\n",
    "svm_study = optuna.create_study(direction='maximize')\n",
    "svm_study.optimize(objective, n_trials=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_svm =SVC(**svm_study.best_params, probability=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Model on Training data\n",
    "train_eval = evaluate_model_train(optimized_svm, X_train, y_train)\n",
    "print(\"Confusion Matrix is: \", train_eval['cm'])\n",
    "print ('Accuracy : ', train_eval['acc'])\n",
    "print('Sensitivity : ', train_eval['sen'])\n",
    "print('Specificity : ', train_eval['spec'])\n",
    "print(\"Mean of Matthews Correlation Coefficient is: \", train_eval['mcc'])\n",
    "print(\"The Precision value is: \", train_eval['prec_train'])\n",
    "print(\"The Recall value is: \", train_eval['recall_train'])\n",
    "print(\"The F1 score is: \", train_eval['f1_train'])\n",
    "print('The area under curve is:', train_eval['AUC'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Model on Testing data\n",
    "dtc_eval = evaluate_model_test(optimized_svm, X_test, y_test)\n",
    "# Print result\n",
    "print('Accuracy:', dtc_eval['acc'])\n",
    "print('Precision:', dtc_eval['prec'])\n",
    "print('Recall:', dtc_eval['rec'])\n",
    "print('F1 Score:', dtc_eval['f1'])\n",
    "print('Area Under Curve:', dtc_eval['auc'])\n",
    "print('Sensitivity : ', dtc_eval['sen'])\n",
    "print('Specificity : ', dtc_eval['spec'])\n",
    "print('MCC Score : ', dtc_eval['mcc'])\n",
    "print('Confusion Matrix:\\n', dtc_eval['cm'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NaII-Pred\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "en_clf = VotingClassifier(estimators=[ ('RF', optimized_RF), ('XGB', optimized_XGB), \n",
    "                                      (\"SVM\", optimized_svm), ('LGBM', optimized_lgbm)], voting='soft')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Evaluate Model on Training data\n",
    "train_eval = evaluate_model_train(en_clf, X_train, y_train)\n",
    "print(\"Confusion Matrix is:\\n\", train_eval['cm'])\n",
    "print ('Accuracy : ', train_eval['acc'])\n",
    "print('Sensitivity : ', train_eval['sen'])\n",
    "print('Specificity : ', train_eval['spec'])\n",
    "print(\"Matthews Correlation Coefficient is: \", train_eval['mcc'])\n",
    "print(\"Precision value is: \", train_eval['prec_train'])\n",
    "print(\"Recall value is: \", train_eval['recall_train'])\n",
    "print(\"F1 score is: \", train_eval['f1_train'])\n",
    "print('The area under curve is:', train_eval['AUC'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate Model on Testing data\n",
    "dtc_eval = evaluate_model_test(en_clf, X_test, y_test)\n",
    "# Print result\n",
    "print('Accuracy:', dtc_eval['acc'])\n",
    "print('Precision:', dtc_eval['prec'])\n",
    "print('Recall:', dtc_eval['rec'])\n",
    "print('F1 Score:', dtc_eval['f1'])\n",
    "print('Area Under Curve:', dtc_eval['auc'])\n",
    "print('Sensitivity : ', dtc_eval['sen'])\n",
    "print('Specificity : ', dtc_eval['spec'])\n",
    "print('MCC Score : ', dtc_eval['mcc'])\n",
    "print('Confusion Matrix:\\n', dtc_eval['cm'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
